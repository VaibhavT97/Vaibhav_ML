{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01d4af0a-2165-446c-ad81-03e9cc01ac19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hp\\miniconda3\\envs\\mlwork\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "@author: MD Gufran Alam; Vaibhav Tripathi; Mohit Prakash Mohanty\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Import important libraris\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scikeras\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, cohen_kappa_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "from skopt import BayesSearchCV\n",
    "from keras.models import Sequential\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#All the required preprocessing was done in GIS softwares\n",
    "\n",
    "#Load the data in csv format \n",
    "data = pd.read_csv('file_path')\n",
    "X, Y = data.drop('target_column', axis=1), data['target_column']\n",
    "\n",
    "# Step 2: Split the extracted subset into training and testing datasets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)\n",
    "\n",
    "# Information Gain Ratio (IGR)\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "information_gain = mutual_info_classif(X_train, Y_train)\n",
    "# Display Information Gain Ratio values\n",
    "igr_values = pd.DataFrame({'Features': X_train.columns, 'Information Gain Ratio': information_gain})\n",
    "sorted_igr_values = igr_values.sort_values(by='Information Gain Ratio', ascending=False)\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(x='Features', y='Information Gain Ratio', data=sorted_igr_values, color='blue')\n",
    "plt.title('Information Gain Ratio (IGR) for Flood Conditioning Factors')\n",
    "plt.xlabel('Flood Conditioning Factors')\n",
    "plt.ylabel('Information Gain Ratio Values')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0.0, 0.5)  # Set y-axis limits to display values from 0.0 to 0.5\n",
    "plt.tight_layout()\n",
    "plt.savefig('file_name.jpg', dpi =1000, bbox_inches ='tight')\n",
    "plt.show()\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "# Calculate Pearson's correlation coefficient\n",
    "correlation_matrix = data.corr()\n",
    "# Display Pearson's correlation coefficient matrix\n",
    "print(\"Correlation Coefficient Matrix:\")\n",
    "\n",
    "# Create a heatmap of the correlation matrix\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "plt.figure(figsize=(18, 14))\n",
    "annot_kws = {\"size\": 11, \"weight\": \"bold\"}  # Change the font size of numeric values\n",
    "cbar_kws = {\"shrink\": 1}  # Adjust the size of the colorbar\n",
    "# Plot the heatmap with modified parameters\n",
    "heatmap = sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap=\"RdBu\", vmin=-1, vmax=1, annot_kws=annot_kws, cbar_kws=cbar_kws)\n",
    "\n",
    "# Loop through the text elements to format their values\n",
    "for text in heatmap.texts:\n",
    "    text.set_text(f'{float(text.get_text()):.2f}')  # Format text to display only three decimal places\n",
    "plt.title(\"Pearson's Correlation Coefficient Matrix\", fontsize=22, fontweight='bold')\n",
    "plt.xlabel(\"Flood Conditioning Factors\", fontsize=20, fontweight='bold')\n",
    "plt.ylabel(\"Flood Conditioning Factors\", fontsize=20, fontweight='bold')\n",
    "plt.savefig('plot.jpg', dpi=1800, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Perform Variance Inflation Factor (VIF) calculation\n",
    "vif_data_new = pd.DataFrame()\n",
    "vif_data_new[\"Features\"] = X_train.columns\n",
    "vif_data_new[\"VIF\"] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n",
    "# Display VIF values\n",
    "vif_data_new.sort_values(by='VIF', ascending=True, inplace=True)\n",
    "vif_data_new\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit the scaler on the training data and transform both the training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train_sf)\n",
    "X_test_scaled = scaler.transform(X_test_sf)\n",
    "# Reshape the data to (n_samples, seq_length, n_features)\n",
    "Reshaped_X_train = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
    "Reshaped_X_test = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
    "\n",
    "\n",
    "\n",
    "# Define LSTM Model\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(64, activation='relu', input_shape=input_shape, return_sequences=True),\n",
    "        LSTM(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Bayesian Hyperparameter Tuning for LSTM\n",
    "lstm_classifier = KerasClassifier(\n",
    "    model=create_lstm_model,\n",
    "    input_shape=(X_train_scaled.shape[1], 1),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "lstm_search = BayesSearchCV(\n",
    "    lstm_classifier,\n",
    "    {\n",
    "        'model__lstm_1__units': (32, 128),\n",
    "        'model__lstm_2__units': (16, 64),\n",
    "        'batch_size': (16, 64),\n",
    "        'model__learning_rate': (0.001, 0.01, 'log-uniform'),\n",
    "        'epochs': (5, 30),\n",
    "    },\n",
    "    n_iter=30,\n",
    "    random_state=42,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "# Reshape input for LSTM\n",
    "X_train_lstm = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
    "X_test_lstm = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
    "\n",
    "lstm_search.fit(X_train_lstm, Y_train)\n",
    "best_lstm_model = lstm_search.best_estimator_\n",
    "\n",
    "# Bayesian Hyperparameter Tuning for RandomForestClassifier\n",
    "rf_search = BayesSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    {\n",
    "        'n_estimators': (50, 500),\n",
    "        'max_depth': (3, 20),\n",
    "        'min_samples_split': (2, 20),\n",
    "        'min_samples_leaf': (1, 10)\n",
    "    },\n",
    "    n_iter=30,\n",
    "    random_state=42,\n",
    "    cv=3\n",
    ")\n",
    "rf_search.fit(X_train_scaled, Y_train)\n",
    "best_rf_model = rf_search.best_estimator_\n",
    "\n",
    "# Bayesian Hyperparameter Tuning for XGBClassifier\n",
    "xgb_search = BayesSearchCV(\n",
    "    XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    {\n",
    "        'n_estimators': (50, 500),\n",
    "        'max_depth': (3, 20),\n",
    "        'learning_rate': (0.01, 0.3, 'log-uniform'),\n",
    "        'subsample': (0.5, 1.0),\n",
    "        'colsample_bytree': (0.5, 1.0)\n",
    "    },\n",
    "    n_iter=30,\n",
    "    random_state=42,\n",
    "    cv=3\n",
    ")\n",
    "xgb_search.fit(X_train_scaled, Y_train)\n",
    "best_xgb_model = xgb_search.best_estimator_\n",
    "\n",
    "# Bayesian Hyperparameter Tuning for CatBoostClassifier\n",
    "cat_search = BayesSearchCV(\n",
    "    CatBoostClassifier(verbose=0, random_state=42),\n",
    "    {\n",
    "        'iterations': (100, 500),\n",
    "        'depth': (3, 10),\n",
    "        'learning_rate': (0.01, 0.3, 'log-uniform'),\n",
    "        'l2_leaf_reg': (1, 10)\n",
    "    },\n",
    "    n_iter=30,\n",
    "    random_state=42,\n",
    "    cv=3\n",
    ")\n",
    "cat_search.fit(X_train_scaled, Y_train)\n",
    "best_cat_model = cat_search.best_estimator_\n",
    "\n",
    "# Bayesian Hyperparameter Tuning for Meta-Learner (XGBClassifier)\n",
    "meta_search = BayesSearchCV(\n",
    "    XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    {\n",
    "        'n_estimators': (50, 500),\n",
    "        'max_depth': (3, 20),\n",
    "        'learning_rate': (0.01, 0.3, 'log-uniform'),\n",
    "        'subsample': (0.5, 1.0),\n",
    "        'colsample_bytree': (0.5, 1.0)\n",
    "    },\n",
    "    n_iter=30,\n",
    "    random_state=42,\n",
    "    cv=3\n",
    ")\n",
    "meta_search.fit(X_train_scaled, Y_train)\n",
    "best_meta_model = meta_search.best_estimator_\n",
    "\n",
    "# Define Base Models for Stacking\n",
    "base_models = [\n",
    "    ('rf', best_rf_model),\n",
    "    ('xgb', best_xgb_model),\n",
    "    ('cb', best_cat_model),\n",
    "    ('lstm', best_lstm_model)\n",
    "]\n",
    "\n",
    "# Stacking Classifier with Optimized Meta-Learner\n",
    "stacked_model = StackingClassifier(estimators=base_models, final_estimator=best_meta_model)\n",
    "stacked_model.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# -------------------\n",
    "# Evaluate Models\n",
    "models = {\n",
    "    'Stacked Model': stacked_model,\n",
    "    'Random Forest': best_rf_model,\n",
    "    'XGBoost': best_xgb_model,\n",
    "    'CatBoost': best_cat_model,\n",
    "    'LSTM': best_lstm_model\n",
    "}\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    predictions = model.predict(X_test_scaled if name != 'LSTM' else X_test_lstm)\n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy_score(Y_test, predictions),\n",
    "        'Precision': precision_score(Y_test, predictions),\n",
    "        'Recall': recall_score(Y_test, predictions),\n",
    "        'F1 Score': f1_score(Y_test, predictions),\n",
    "        'Cohen Kappa': cohen_kappa_score(Y_test, predictions)\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"Evaluation Metrics:\\n\", results_df)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize an empty dictionary to store confusion matrices\n",
    "confusion_matrices = {}\n",
    "\n",
    "# Iterate over models to calculate confusion matrices\n",
    "for name, model in models.items():\n",
    "    predictions = model.predict(X_test_scaled if name != 'LSTM' else X_test_lstm)\n",
    "    cm = confusion_matrix(Y_test, predictions)\n",
    "    confusion_matrices[name] = cm  # Store the confusion matrix\n",
    "    \n",
    "    # Plot each confusion matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Not Flooded', 'Flooded'], \n",
    "                yticklabels=['Not Flooded', 'Flooded'])\n",
    "    plt.title(f'Confusion Matrix - {name}')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{name}_confusion_matrix.jpg', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Print confusion matrices for quick reference\n",
    "for name, cm in confusion_matrices.items():\n",
    "    print(f\"\\nConfusion Matrix for {name}:\\n{cm}\")\n",
    "\n",
    "\n",
    "# Predict probabilities using the modelS\n",
    "rf_test_pred_prob = best_rf_model.predict_proba(Std_scaled_test)[:,1]\n",
    "xgb_test_pred_prob = best_xgb_model.predict_proba(Std_scaled_test)[:,1]\n",
    "cb_test_pred_prob = best_cb_model.predict_proba(Std_scaled_test)[:,1]\n",
    "ensemble_test_pred_prob = ensemble_model_new.predict_proba(Std_scaled_test)[:, 1]\n",
    "\n",
    "#Reshape the output to actual spatial grid\n",
    "susceptibility_map = ensemble_test_pred_prob.reshape(n_rows, n_cols) #We got ensemble_test_pred_prob as the best model based on performance metrics\n",
    "# Visualize the susceptibility map\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(susceptibility_map, cmap='RdYlGn', interpolation='none')\n",
    "plt.colorbar(label='Susceptibility Score')\n",
    "plt.title('Flood Susceptibility Map')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.savefig('flood_susceptibility_map.jpg', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import shap\n",
    "from shap import summary_plot\n",
    "\n",
    "# Initialize JS visualization for SHAP\n",
    "shap.initjs()\n",
    "# Prepare a dictionary to store SHAP values and explainers for each model\n",
    "shap_results = {}\n",
    "# Define a list of models for which SHAP analysis will be performed\n",
    "model_names = ['Random Forest', 'XGBoost', 'CatBoost', 'LSTM', 'Stacked Model']\n",
    "models_dict = {\n",
    "    'Random Forest': best_rf_model,\n",
    "    'XGBoost': best_xgb_model,\n",
    "    'CatBoost': best_cat_model,\n",
    "    'LSTM': best_lstm_model,\n",
    "    'Stacked Model': stacked_model\n",
    "}\n",
    "\n",
    "# SHAP Analysis for Each Model\n",
    "for name, model in models_dict.items():\n",
    "    print(f\"Generating SHAP values for {name}...\")\n",
    "    \n",
    "    if name == 'LSTM':  # For LSTM model\n",
    "        explainer = shap.DeepExplainer(model.model_, X_train_lstm)  # Use DeepExplainer for neural networks\n",
    "        shap_values = explainer.shap_values(X_test_lstm)\n",
    "    else:  # For tree-based and stacking models\n",
    "        explainer = shap.TreeExplainer(model, X_train_scaled)\n",
    "        shap_values = explainer.shap_values(X_test_scaled)\n",
    "    \n",
    "    # Store SHAP results\n",
    "    shap_results[name] = {'explainer': explainer, 'shap_values': shap_values}\n",
    "    \n",
    "    # Generate Summary Plot\n",
    "    print(f\"Creating SHAP summary plot for {name}...\")\n",
    "    summary_plot(shap_values, X_test_scaled if name != 'LSTM' else X_test_lstm, feature_names=X.columns)\n",
    "    plt.savefig(f'{name}_shap_summary_plot.jpg', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f95f24e-56e0-480d-b997-023b42ecfeeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
